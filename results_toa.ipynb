{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import Any, cast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from models import models, scalers\n",
    "from read_data import read_datasets\n",
    "from data_cleaning import prep_dataframe, DataCleaner\n",
    "from training import train_test_random, split_x_y, calc_stats"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-05 15:10:50.706815: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Datasets\n",
    "\n",
    "Copies of the dataset, each with a different scaler applies, are generated and stored for usage in training."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dep_var = \"Log(Efficiency)\"\n",
    "\n",
    "all_data = read_datasets()\n",
    "combined_data = prep_dataframe(all_data, dep_var)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TOP500_201906.xls\n",
      "TOP500_202011.xlsx\n",
      "TOP500_201911.xls\n",
      "TOP500_202006.xlsx\n",
      "TOP500_201811.xls\n",
      "TOP500_201806.xls\n",
      "TOP500_201206.xls\n",
      "TOP500_201211.xls\n",
      "TOP500_201406.xls\n",
      "TOP500_202106.xlsx\n",
      "TOP500_201611.xls\n",
      "TOP500_201411.xls\n",
      "TOP500_201606.xls\n",
      "TOP500_201311.xls\n",
      "TOP500_201306.xls\n",
      "TOP500_201111.xls\n",
      "TOP500_201511.xls\n",
      "TOP500_201706.xls\n",
      "TOP500_201506.xls\n",
      "TOP500_201711.xls\n",
      "Unknown processor: 'NEC', full name: 'NEC  3.200GHz' @ Earth Simulator, 2009\n",
      "Unknown processor: 'NEC', full name: 'NEC  3.200GHz' @ Earth Simulator, 2009\n",
      "Unknown processor: 'NEC', full name: 'NEC  3.200GHz' @ Earth Simulator, 2009\n",
      "Unknown processor: 'NEC', full name: 'NEC  3.200GHz' @ Earth Simulator, 2009\n",
      "Unknown processor: 'NEC', full name: 'NEC  3.20GHz' @ Earth Simulator, 2009\n",
      "Unknown processor: 'Xeon EM64T', full name: 'Xeon EM64T  3.60GHz' @ Thunderbird, 2006\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "datasets = {}\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    data = DataCleaner(scaler, dep_var).fit_transform(combined_data.copy())\n",
    "\n",
    "    non_holdout, holdout = train_test_random(data, 0.1)\n",
    "    train, test = train_test_random(non_holdout, 0.1)\n",
    "\n",
    "    # Do splits for all data\n",
    "    datasets[scaler_name] = split_x_y([train, test, holdout], dep_var)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Filtered duplicates to go from 5332 rows to 1530\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4v/ps0_pwf52rd0x9m9hjkntgbh0000gn/T/ipykernel_11707/793705922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscaler_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscalers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataCleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdep_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnon_holdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = pd.DataFrame(columns=[\"name\", \"scaler\", \"r2\", \"mae\", \"mape\", \"mse\"])\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    for scaler_name in scalers.keys():\n",
    "        train, test, holdout = datasets[scaler_name]\n",
    "        model.fit(train[0], train[1])\n",
    "\n",
    "        pred_y = model.predict(test[0])\n",
    "        result = calc_stats(test[1], pred_y, print_res=False)\n",
    "        result = cast(dict[str, Any], result)\n",
    "        result[\"name\"] = model_name\n",
    "        result[\"scaler\"] = scaler_name\n",
    "\n",
    "        results = results.append(result, ignore_index=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results.to_csv(\"results.csv\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Take only the maximum scaler config for each model\n",
    "max_indices = results.groupby([\"name\"])[\"r2\"].idxmax()\n",
    "maximums = results.loc[max_indices]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "maximums.to_csv(\"results.csv\")\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "32c98c724f4dc158d685d7f352a5521c40874805d8d14b99c4302dfd329e583c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}